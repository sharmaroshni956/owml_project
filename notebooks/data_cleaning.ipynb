{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187c255b",
   "metadata": {},
   "source": [
    "# Unified Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d530e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re                                   # to preprocess regular expressions\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords           #  to remove common words to reduce noise\n",
    "from nltk.stem import WordNetLemmatizer     # to reduce words to their root form like 'running' to 'run'\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a03c401",
   "metadata": {},
   "source": [
    "### NLTK Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf6294b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/roshni/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/roshni/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading NLTK resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfdfa75",
   "metadata": {},
   "source": [
    "### Base Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "040c816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))       # Automatically detects current project directory\n",
    "\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")                         # Define subdirectories relative to project root\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"output\")                     # optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0986173",
   "metadata": {},
   "source": [
    "### Initializing Stopwords and Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d42383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc7eddd",
   "metadata": {},
   "source": [
    "### Function to Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6b28684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text): \n",
    "    text = str(text).lower()                         \n",
    "    text = re.sub(r\"http\\S+\", \"\", text)                            # removes URLs\n",
    "    text = re.sub(r'<.*?>', '', text)                              # removes HTML tags\n",
    "    text = re.sub(r'\\d+', '', text)                                # removes digits\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)  # removes punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()                       # removes extra spaces\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):                                         # Tokenization and Lemmatization\n",
    "    words = text.split()                                           # Tokenization by splitting on spaces\n",
    "    words = [w for w in words if w not in stop_words]              # Remove stopwords\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]               # Lemmatization by reducing words to their root form\n",
    "    return ' '.join(words)                                         # Join words back into a single string "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4681a66",
   "metadata": {},
   "source": [
    "### Dataset Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74dd58cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df, text_col, label_col, dataset_name):           \n",
    "    df = df[[text_col, label_col]].copy()                                        # Keep only relevant columns\n",
    "    df.rename(columns={text_col: 'text', label_col: 'label'}, inplace=True)      # Standardize column names\n",
    "\n",
    "    df.drop_duplicates(inplace=True)                                             # Remove duplicates\n",
    "    df.dropna(subset=['text'], inplace=True)                                     # Remove rows with missing text\n",
    "    df.reset_index(drop=True, inplace=True)                                      # Reset index\n",
    "\n",
    "    df['text'] = df['text'].apply(clean_text)                                    # applying cleaning function\n",
    "    df['text'] = df['text'].apply(preprocess_text)                               # applying Preprocessing function\n",
    "\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"{dataset_name}_Cleaned.csv\")        # define output path\n",
    "    df.to_csv(output_path, index=False)                                          # Save cleaned CSV\n",
    "    print(f\"✅ {dataset_name} cleaned and saved ({df.shape[0]} rows) → {output_path}\")    \n",
    "    return df                                                                    # return cleaned dataframe    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bba673",
   "metadata": {},
   "source": [
    "## Load & Clean Each Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6947bb58",
   "metadata": {},
   "source": [
    "### CoAID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4b11c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full path: /Users/roshni/Desktop/DrParmar_Project/owml_project/notebooks/data/CoAID/CoAID_News_Combined.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Full path:\", coaid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bec4c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CoAID cleaned and saved (3079 rows) → /Users/roshni/Desktop/DrParmar_Project/owml_project/output/CoAID_Cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "coaid_path = os.path.join(DATA_DIR, \"CoAID\", \"CoAID_News_Combined.csv\")                                        # path to CoAID dataset\n",
    "coaid_df = pd.read_csv(coaid_path)                                                                          # load CoAID dataset\n",
    "coaid_cleaned = clean_dataset(coaid_df, text_col='content', label_col='label', dataset_name='CoAID')        # apply cleaning function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51099b3",
   "metadata": {},
   "source": [
    "### FakeNewsNet (PolitiFact example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcc39996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FakeNewsNet cleaned and saved (228 rows) → /Users/roshni/Desktop/DrParmar_Project/owml_project/output/FakeNewsNet_Cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "fakenews_path_real = os.path.join(DATA_DIR, \"FakeNewsNet/PolitiFact_real_news_content.csv\")                      \n",
    "fakenews_path_fake = os.path.join(DATA_DIR, \"FakeNewsNet/PolitiFact_fake_news_content.csv\")                      \n",
    "df_real = pd.read_csv(fakenews_path_real)                                                                        \n",
    "df_fake = pd.read_csv(fakenews_path_fake)                                                                        \n",
    "\n",
    "df_fakenews = pd.concat([df_real, df_fake], ignore_index=True)                                                   # combine real and fake news\n",
    "\n",
    "df_fakenews['label'] = [1]*len(df_fake) + [0]*len(df_real)                                                       # create label column: 1 for fake, 0 for real\n",
    "\n",
    "fakenews_cleaned = clean_dataset(df_fakenews, text_col='text', label_col='label', dataset_name='FakeNewsNet')    # apply cleaning function\n",
    "\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7622b034",
   "metadata": {},
   "source": [
    "### WELFake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f01a02e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ WELFake cleaned and saved (63678 rows) → /Users/roshni/Desktop/DrParmar_Project/owml_project/output/WELFake_Cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "welfare_path = os.path.join(DATA_DIR, \"WELFake/WELFake.csv\")                                                      \n",
    "welfare_df = pd.read_csv(welfare_path)                                                                            \n",
    "\n",
    "welfare_df['text_combined'] = welfare_df['title'].fillna('') + ' ' + welfare_df['text'].fillna('')                # combine title and text columns\n",
    "welfare_cleaned = clean_dataset(welfare_df, text_col='text_combined', label_col='label', dataset_name='WELFake')  # apply cleaning function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b608f1b",
   "metadata": {},
   "source": [
    "## Quick Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e75feaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample cleaned text from CoAID:\n",
      "0    browser longer supported please switch support...\n",
      "1    de moines iowa new study conducted researcher ...\n",
      "2    confirm receive notification channel subscribe...\n",
      "Name: text, dtype: object\n",
      "\n",
      "Sample cleaned text from FakeNewsNet:\n",
      "0    k share share story hillary clinton called fac...\n",
      "1    famous dog killed spot waited year owner retur...\n",
      "2    story highlight house oversight panel voted ho...\n",
      "Name: text, dtype: object\n",
      "\n",
      "Sample cleaned text from WELFake:\n",
      "0    law enforcement high alert following threat co...\n",
      "1                            post vote hillary already\n",
      "2    unbelievable obama’s attorney general say char...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample cleaned text from CoAID:\")              # print sample cleaned text from CoAID\n",
    "print(coaid_cleaned.head(3)['text'])\n",
    "\n",
    "print(\"\\nSample cleaned text from FakeNewsNet:\")        # print sample cleaned text from FakeNewsNet\n",
    "print(fakenews_cleaned.head(3)['text'])\n",
    "\n",
    "print(\"\\nSample cleaned text from WELFake:\")            # print sample cleaned text from WELFake\n",
    "print(welfare_cleaned.head(3)['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16da97a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "owml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
