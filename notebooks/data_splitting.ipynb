{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74b8be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0204b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"  \n",
    "SPLIT_DIR = \"../data/split\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c06ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"CoAID\": os.path.join(DATA_DIR, \"CoAID_Cleaned.csv\"),\n",
    "    \"FakeNewsNet\": os.path.join(DATA_DIR, \"FakeNewsNet_Cleaned.csv\"),\n",
    "    \"WELFake\": os.path.join(DATA_DIR, \"WELFake_Cleaned.csv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed8528",
   "metadata": {},
   "source": [
    "## Standard Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e080d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_split(df, label_col=\"label\", test_size=0.2, val_size=0.1, random_state=42):                                # Function to split dataset into train, validation, and test sets\n",
    "    train_df, temp_df = train_test_split(df, test_size=test_size, random_state=random_state, stratify=df[label_col])    # Splitting into train and temp (val+test)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=random_state, stratify=temp_df[label_col])  # Splitting temp into val and test\n",
    "    return train_df, val_df, test_df        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1650ce",
   "metadata": {},
   "source": [
    "## Iterate Over Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7a7ec11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing CoAID dataset...\n",
      "Saved standard splits: train(2463), val(308), test(308)\n",
      "\n",
      "Processing FakeNewsNet dataset...\n",
      "Saved standard splits: train(182), val(23), test(23)\n",
      "\n",
      "Processing WELFake dataset...\n",
      "Saved standard splits: train(50942), val(6368), test(6368)\n",
      "\n",
      "All datasets processed and saved under 'data/split/'\n"
     ]
    }
   ],
   "source": [
    "for name, path in datasets.items():                                             # Iterating over each dataset\n",
    "    print(f\"\\nProcessing {name} dataset...\")    \n",
    "    df = pd.read_csv(path)              \n",
    "    os.makedirs(os.path.join(SPLIT_DIR, name), exist_ok=True)                   # Creating directory for splits if not exists\n",
    "    \n",
    "    train_df, val_df, test_df = standard_split(df, label_col=\"label\")           # Performing standard split\n",
    "    \n",
    "    train_df.to_csv(os.path.join(SPLIT_DIR, name, \"train.csv\"), index=False)    # Saving splits to CSV files\n",
    "    val_df.to_csv(os.path.join(SPLIT_DIR, name, \"val.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(SPLIT_DIR, name, \"test.csv\"), index=False)\n",
    "    \n",
    "    print(f\"Saved standard splits: train({len(train_df)}), val({len(val_df)}), test({len(test_df)})\")\n",
    "\n",
    "print(\"\\nAll datasets processed and saved under 'data/split/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592ef39b",
   "metadata": {},
   "source": [
    "## Advanced Open-World Setup (Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12753f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_DIR = \"../data/split\"                        # Already contains standard splits\n",
    "datasets = [\"CoAID\", \"FakeNewsNet\", \"WELFake\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c80e38",
   "metadata": {},
   "source": [
    "## Advanced Open-World Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb37fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_open_world_split(df, source_col=\"source\", topic_col=\"category\", unseen_ratio=0.2, random_state=42):     # Function to create open-world splits\n",
    "    sources = df[source_col].unique()                                                                                # Unique sources in the dataset\n",
    "    train_sources, unseen_sources = train_test_split(sources, test_size=unseen_ratio, random_state=random_state)     # Splitting sources into seen and unseen\n",
    "    \n",
    "    topics = df[topic_col].unique()                                                                                  # Unique topics in the dataset\n",
    "    train_topics, unseen_topics = train_test_split(topics, test_size=unseen_ratio, random_state=random_state)        # Splitting topics into seen and unseen\n",
    "    \n",
    "    ow_train = df[df[source_col].isin(train_sources) & df[topic_col].isin(train_topics)].reset_index(drop=True)      # Training set with seen sources and topics\n",
    "    ow_test = df[(df[source_col].isin(unseen_sources)) | (df[topic_col].isin(unseen_topics))].reset_index(drop=True) # Test set with unseen sources or topics\n",
    "    \n",
    "    return ow_train, ow_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edd6bc4",
   "metadata": {},
   "source": [
    "## Iterate Over Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcb2ab1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing CoAID dataset for advanced open-world setup...\n",
      "Saved advanced open-world splits: train(1968), test(495)\n",
      "\n",
      "Processing FakeNewsNet dataset for advanced open-world setup...\n",
      "Saved advanced open-world splits: train(148), test(34)\n",
      "\n",
      "Processing WELFake dataset for advanced open-world setup...\n",
      "Saved advanced open-world splits: train(40753), test(10189)\n",
      "\n",
      "Advanced open-world setup complete!\n"
     ]
    }
   ],
   "source": [
    "for name in datasets:                                                                               # Iterating over each dataset\n",
    "    print(f\"\\nProcessing {name} dataset for advanced open-world setup...\")\n",
    "    path = os.path.join(SPLIT_DIR, name, \"train.csv\")                                               # use standard train as base\n",
    "    df = pd.read_csv(path)                                                                          # Loading the training data\n",
    "    \n",
    "    source_col = \"source\" if \"source\" in df.columns else df.columns[0]                              # Determining source column\n",
    "    topic_col = \"category\" if \"category\" in df.columns else df.columns[0]                           # Determining topic column\n",
    "    \n",
    "    ow_train, ow_test = advanced_open_world_split(df, source_col=source_col, topic_col=topic_col)   # Creating open-world splits\n",
    "    \n",
    "    ow_train.to_csv(os.path.join(SPLIT_DIR, name, \"open_world_train.csv\"), index=False)             # Saving open-world splits\n",
    "    ow_test.to_csv(os.path.join(SPLIT_DIR, name, \"open_world_test.csv\"), index=False)               # Saving open-world splits\n",
    "    \n",
    "    print(f\"Saved advanced open-world splits: train({len(ow_train)}), test({len(ow_test)})\")        # Logging the sizes\n",
    "\n",
    "print(\"\\nAdvanced open-world setup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "owml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
